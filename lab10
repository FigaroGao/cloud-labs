Deploying a Predictive Model with KServe on Minikube

1. Install and configure KServe on a local Kubernetes cluster (Minikube)

2. Deploy a predictive model using a KServe InferenceService

3. Monitor the status of the service and debug common issues

4. Find out how to reach the InferenceService from outside the cluster.

5. Send inference requests and interpret results


Related links:

https://kserve.github.io/website/docs/getting-started/quickstart-guide
https://kserve.github.io/website/docs/getting-started/predictive-first-isvc
https://futurelei.com/KServe.mov

You are free to choose your own predictive model.

Please write a document with screenshots and notes for major steps and commit it to your github repo.
